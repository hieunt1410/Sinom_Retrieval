{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q gdown faiss-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gdown --id 1_9NPC8E-kt4pU6rTi5lkh91eFR_jnFav\n",
    "!unzip -q /kaggle/working/wb_2D3Dretrieval_dataset.zip\n",
    "!rm /kaggle/working/wb_2D3Dretrieval_dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logging import currentframe\n",
    "import numpy as np\n",
    "from stl import mesh\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import os\n",
    "from matplotlib.colors import Normalize\n",
    "from scipy.interpolate import griddata\n",
    "import cv2 as cv\n",
    "import pandas as pd\n",
    "from keras.models import Model\n",
    "from keras import layers, Input\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path\n",
    "path = '/kaggle/working/wb_2D3Dretrieval_dataset'\n",
    "queries_path = os.path.join(path, 'queries')\n",
    "database3D_2D_path = os.path.join(path, 'database_2D')\n",
    "\n",
    "\n",
    "if not os.path.exists(database3D_2D_path):\n",
    "    os.mkdir(database3D_2D_path)\n",
    "\n",
    "for obj in os.listdir(os.path.join(path, 'database')):\n",
    "    obj_name = obj.split('.')[0]\n",
    "\n",
    "    your_mesh = mesh.Mesh.from_file(os.path.join(path, f'database/{obj}'))\n",
    "\n",
    "    # Get center of gravity\n",
    "#     cog = np.mean(your_mesh.vectors.reshape(-1, 3), axis=0)\n",
    "    cog = [np.max(your_mesh.x) * 0.5, np.max(your_mesh.y) * 0.5, np.max(your_mesh.z) * 0.5]\n",
    "\n",
    "    # Define the equation of the cutting plane\n",
    "    normal = np.array([0, 0, 1])  # normal vector of the plane\n",
    "\n",
    "    intersection_points = []\n",
    "    for i in range(len(your_mesh.vectors)):\n",
    "        for j in range(3):\n",
    "            v0 = your_mesh.vectors[i, j - 1]\n",
    "            v1 = your_mesh.vectors[i, j]\n",
    "            if np.dot(normal, v0 - cog) * np.dot(normal, v1 - cog) < 0:\n",
    "                t = np.dot(normal, cog - v0) / np.dot(normal, v1 - v0)\n",
    "                intersection_points.append(v0 + t * (v1 - v0))\n",
    "\n",
    "    # Convert intersection points to NumPy array\n",
    "    above_plane_vertices = your_mesh.vectors[your_mesh.vectors[:, :, 2] > cog[2]]\n",
    "\n",
    "    # Flatten the vertices and remove duplicates\n",
    "    # intersection_points = np.unique(above_plane_vertices.reshape(-1, 3), axis=0)\n",
    "    intersection_points = above_plane_vertices\n",
    "\n",
    "    # Convert intersection points to NumPy array\n",
    "    intersection_points = np.array(intersection_points)\n",
    "\n",
    "    # Project 3D vertices onto 2D plane (discard z-coordinate)\n",
    "    projection_points = intersection_points[:, :2]\n",
    "\n",
    "    # Calculate grayscale values based on z-coordinate\n",
    "    z_values = intersection_points[:, 2]\n",
    "    try:\n",
    "      min_z, max_z = np.min(z_values), np.max(z_values)\n",
    "    except ValueError as e:\n",
    "      print(obj)\n",
    "\n",
    "    # Invert grayscale values\n",
    "    inverted_values = max_z - z_values\n",
    "\n",
    "    # Define grid for interpolation\n",
    "    x_grid = np.linspace(min(projection_points[:, 0]), max(projection_points[:, 0]), 200)\n",
    "    y_grid = np.linspace(min(projection_points[:, 1]), max(projection_points[:, 1]), 200)\n",
    "    xx, yy = np.meshgrid(x_grid, y_grid)\n",
    "\n",
    "    # Perform bilinear interpolation\n",
    "    interpolated_values = griddata(projection_points, inverted_values, (xx, yy), method='linear')\n",
    "\n",
    "    # Normalize interpolated values\n",
    "    norm = Normalize(vmin=min_z, vmax=max_z)\n",
    "    grayscale_values = norm(interpolated_values)\n",
    "\n",
    "    plt.figure(figsize=(5.12, 5.12))\n",
    "\n",
    "    rotated_grayscale_values = np.rot90(grayscale_values, k=2)  # Rotate 180 degrees\n",
    "\n",
    "    flipped_grayscale_values = np.fliplr(rotated_grayscale_values)  # Horizontal flip\n",
    "\n",
    "    plt.imshow(flipped_grayscale_values, extent=(min(x_grid), max(x_grid), min(y_grid), max(y_grid)), cmap='gray')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.savefig(os.path.join(database3D_2D_path, f'/{obj_name}.png'))\n",
    "\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = keras.Input(shape=(512, 512, 1))\n",
    "\n",
    "x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)\n",
    "x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = layers.Conv2D(16, (3, 3), activation='relu', padding='same')(x)\n",
    "x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = layers.Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "encoded = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "# at this point the representation is (4, 4, 8) i.e. 128-dimensional\n",
    "\n",
    "x = layers.Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)\n",
    "x = layers.UpSampling2D((2, 2))(x)\n",
    "x = layers.Conv2D(16, (3, 3), activation='relu', padding='same')(x)\n",
    "x = layers.UpSampling2D((2, 2))(x)\n",
    "x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "x = layers.UpSampling2D((2, 2))(x)\n",
    "decoded = layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "autoencoder = keras.Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "for i in os.listdir(queries_path):\n",
    "  x_train.append(cv.imread(os.path.join(queries_path, i), cv.IMREAD_GRAYSCALE))\n",
    "\n",
    "for i in os.listdir(database3D_2D_path):\n",
    "  y_train.append(cv.imread(os.path.join(database3D_2D_path, i), cv.IMREAD_GRAYSCALE))\n",
    "\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "y_train = y_train.astype('float32') / 255.\n",
    "\n",
    "x_train = np.reshape(x_train, (len(x_train), 512, 512, 1))\n",
    "y_train = np.reshape(y_train, (len(y_train), 512, 512, 1))\n",
    "\n",
    "x_test = x_train\n",
    "y_test = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.fit(x_train, y_train,\n",
    "                epochs=20,\n",
    "                batch_size=64,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_imgs = autoencoder.predict(x_test)\n",
    "\n",
    "n = 10\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(1, n + 1):\n",
    "    # Display original\n",
    "    ax = plt.subplot(2, n, i)\n",
    "    plt.imshow(x_test[i].reshape(512, 512))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # Display reconstruction\n",
    "    ax = plt.subplot(2, n, i + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(512, 512))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(image_path, model):\n",
    "    img = cv.imread(image_path, cv.IMREAD_GRAYSCALE)\n",
    "    img = np.reshape(img, (1, 512, 512, 1))\n",
    "    encoder = Model(model.input, model.layers[6].output)\n",
    "    features = encoder.predict(img)\n",
    "    return features.flatten()\n",
    "\n",
    "query_mapping = {}\n",
    "\n",
    "training_features = []\n",
    "training_files = []\n",
    "\n",
    "for file in os.listdir(database3D_2D_path):\n",
    "    file_path = os.path.join(database3D_2D_path, file)\n",
    "    features = extract_features(file_path, autoencoder)\n",
    "    training_features.append(features)\n",
    "    training_files.append(file)\n",
    "\n",
    "# Flatten the list of descriptors and create an index\n",
    "training_features = np.vstack(training_features).astype(np.float32)\n",
    "index = faiss.IndexFlatL2(training_features.shape[1])\n",
    "index.add(training_features)\n",
    "\n",
    "# Function to find top-k matches\n",
    "def find_top_k_matches(features, k=5):\n",
    "    features = features.astype(np.float32)\n",
    "    D, I = index.search(np.expand_dims(features, axis=0), k)\n",
    "    return I[0]\n",
    "\n",
    "mrr_at_5 = 0\n",
    "\n",
    "for query in os.listdir(queries_path):\n",
    "    query_obj = query.split('.')[0]\n",
    "    query_path_full = os.path.join(query_path, query)\n",
    "    query_features = extract_features(query_path_full, autoencoder)\n",
    "\n",
    "    # Find top-5 matches\n",
    "    indices = find_top_k_matches(query_features, k=5)\n",
    "\n",
    "    scores = [(i, training_files[i].split('.')[0] + '.stl') for i in indices]\n",
    "\n",
    "    query_mapping[query] = ''\n",
    "    for i in range(5):\n",
    "        query_mapping[query] += scores[i][1] + ','\n",
    "        if scores[i][1].split('.')[0] == query_obj:\n",
    "            mrr_at_5 += 1 / (i + 1)\n",
    "    query_mapping[query] = query_mapping[query][:-1]\n",
    "\n",
    "    print(query, query_mapping[query])\n",
    "\n",
    "queries_len = len(os.listdir(query_path))\n",
    "print('MRR@5: ', mrr_at_5 / queries_len)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
