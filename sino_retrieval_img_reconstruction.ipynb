{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv\n",
    "import pandas as pd\n",
    "import os\n",
    "from keras.models import Model\n",
    "from keras import layers, Input\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "input_img = keras.Input(shape=(512, 512, 1))\n",
    "\n",
    "# Encoder\n",
    "x = layers.Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = layers.Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = layers.Conv2D(4, (3, 3), activation='relu', padding='same')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "encoded = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "# Decoder\n",
    "x = layers.Conv2D(4, (3, 3), activation='relu', padding='same')(encoded)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.UpSampling2D((2, 2))(x)\n",
    "x = layers.Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.UpSampling2D((2, 2))(x)\n",
    "x = layers.Conv2D(16, (3, 3), activation='relu', padding='same')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.UpSampling2D((2, 2))(x)\n",
    "decoded = layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "autoencoder = Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/kaggle/working'\n",
    "\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "for i in os.listdir(os.path.join(path, 'queries')):\n",
    "  x_train.append(cv.imread(os.path.join(path, 'queries', i), cv.IMREAD_GRAYSCALE))\n",
    "\n",
    "for i in os.listdir(os.path.join(path, 'database_2D')):\n",
    "  y_train.append(cv.imread(os.path.join(path, 'database_2D', i), cv.IMREAD_GRAYSCALE))\n",
    "\n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "y_train = y_train.astype('float32') / 255.\n",
    "\n",
    "x_train = np.reshape(x_train, (len(x_train), 512, 512, 1))\n",
    "y_train = np.reshape(y_train, (len(y_train), 512, 512, 1))\n",
    "\n",
    "x_test = x_train\n",
    "y_test = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.fit(x_train, y_train,\n",
    "                epochs=50,\n",
    "                batch_size=64,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_imgs = autoencoder.predict(x_test)\n",
    "\n",
    "n = 10\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(1, n + 1):\n",
    "    # Display original\n",
    "    ax = plt.subplot(2, n, i)\n",
    "    plt.imshow(x_test[i].reshape(512, 512))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # Display reconstruction\n",
    "    ax = plt.subplot(2, n, i + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(512, 512))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(image_path, model):\n",
    "    img = cv.imread(image_path, cv.IMREAD_GRAYSCALE)\n",
    "    img = np.reshape(img, (1, 512, 512, 1))\n",
    "    encoder = Model(model.input, model.layers[6].output)\n",
    "    features = encoder.predict(img)\n",
    "    return features.flatten()\n",
    "\n",
    "# Load and extract features for training images\n",
    "training_path = 'database_2D'\n",
    "query_path = 'queries'\n",
    "query_mapping = {}\n",
    "\n",
    "training_features = []\n",
    "training_files = []\n",
    "\n",
    "for file in os.listdir(training_path):\n",
    "    file_path = os.path.join(training_path, file)\n",
    "    features = extract_features(file_path, autoencoder)\n",
    "    training_features.append(features)\n",
    "    training_files.append(file)\n",
    "\n",
    "# Flatten the list of descriptors and create an index\n",
    "training_features = np.vstack(training_features).astype(np.float32)\n",
    "index = faiss.IndexFlatL2(training_features.shape[1])\n",
    "index.add(training_features)\n",
    "\n",
    "# Function to find top-k matches\n",
    "def find_top_k_matches(features, k=5):\n",
    "    features = features.astype(np.float32)\n",
    "    D, I = index.search(np.expand_dims(features, axis=0), k)\n",
    "    return I[0]\n",
    "\n",
    "mrr_at_5 = 0\n",
    "\n",
    "for query in os.listdir(query_path):\n",
    "    query_obj = query.split('.')[0]\n",
    "    query_path_full = os.path.join(query_path, query)\n",
    "    query_features = extract_features(query_path_full, autoencoder)\n",
    "\n",
    "    # Find top-5 matches\n",
    "    indices = find_top_k_matches(query_features, k=5)\n",
    "\n",
    "    scores = [(i, training_files[i].split('.')[0] + '.stl') for i in indices]\n",
    "\n",
    "    query_mapping[query] = '\"'\n",
    "    for i in range(5):\n",
    "        query_mapping[query] += scores[i][1] + ','\n",
    "        if scores[i][1].split('.')[0] == query_obj:\n",
    "            mrr_at_5 += 1 / (i + 1)\n",
    "    query_mapping[query] = query_mapping[query][:-1] + '\"'\n",
    "\n",
    "    print(query, query_mapping[query])\n",
    "\n",
    "queries_len = len(os.listdir(query_path))\n",
    "print('MRR@5: ', mrr_at_5 / queries_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {\n",
    "    'query': query_mapping.keys(),\n",
    "    'label': query_mapping.values()\n",
    "}\n",
    "\n",
    "df = pd.DataFrame.from_dict(output)\n",
    "df.to_csv('predict.csv', sep=',', index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
